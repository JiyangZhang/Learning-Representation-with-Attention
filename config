[hyperparams]
model_name_str = qfdRM_f1
fold = 1
batch_size = 128
vocab_size = 400002
emb_size = 300
hidden_size = 256
dropout = 0
preemb = False
sim_type = cossim
hinge_margin = 1.0
emb_tune = True
n_epoch = 50
eval_every_num_update = 8
alpha = 1e-5
beta1 = 0.9
beta2 = 0.98
learning_rate = 1e-3
num_heads = 1
emb_path = /data/rali5/Tmp/nieyifan/aol/glove_emb.pkl
q_len = 15
d_len = 1000
data_base_path = /data/rali7/Tmp/nieyifan/MQ/
model_base_path = /part/01/Tmp/Jiyang
dataset = MQ
q_sample_size = 100
docpair_sample_size = 500

[rep]
filt_size = 256
kernel_size = 3
output_dim = 128





